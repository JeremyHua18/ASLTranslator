# American Sign Language Translator
Project for "CS4476 - Computer Vision" by Dheeraj Bandaru, Sichong Hua, Jing Xi Liu, Farouk Marhaba, and Kira Pancha.

## Problem Statement
Our team is working on a project that aims to parse American Sign Language (ASL) letters from a photo or video feed and convert it to text. Across different forms of communication, the mute community’s accessibility concerns have been historically underrepresented. Our project aims to provide an alternative for “speech-to-text” softwares for the mute community by giving users the ability to quickly and efficiently translate ASL letters to traditional English text.

The expected input into the system can be an image (single frame) where an ASL letter is present. Alternatively, it could be a video (multiple frames) where a string of ASL letters is parsed one letter at a time. After the frame(s) get processed through the translator, the corresponding English letters would be produced. The final output generated from the raw output can be represented in various forms, such as readable text, emojis, or even short-clipped audio.

## Approach
The technical approach we will take to get the results we desire will include building a classification model to differentiate between the different ASL signs. We will then build a series of image preprocessing methods implementing various Computer Vision techniques to enhance the user input for our model input, and finally create an output which will consist of a string of letters based on the user input. The classification model we plan to build to classify the different ASL signs will be the pretrained Inception V3 model which is a convolutional neural net that has been shown to classify images better and more efficiently than other widely available models like ImageNet. Inception V3 model is also easily implementable and can be trained on the datasets we find most relevant to our project. The next step will be to build the image preprocessing model which is where we will be experimenting with various different filters such as the Prewitt, Sobel, and Gaussian filters. We will also experiment with various edge detection methods and extraction methods like the graph-cut algorithm to better isolate the hand symbol and allow for better classification in our model. We will also use energy minimization techniques to allow for hands in different angles and lighting to be inputted and get reasonable outputs. The classifier model which will be used as a testing apparatus for our Computer Vision experiments we will have a baseline accuracy score based on just training with non-processed images and then we will run experiments using some of the various image preprocessing methods listed above to determine the best way we can improve the results of this device from a Computer Vision standpoint. 

## Experiments and Results
The experimental setup will consist of baseline results from our Inception V3 model being inputted the dataset we specify later on in the proposal entitled Sign Language MNIST. This dataset will be split into a test and train set that will give us a rough idea of how perfect image inputs would perform with our classifier. Then we will collect our own data. The data collection protocol we will use will be by taking pictures using our smartphones of two of our teammates making the 26 letters in the Sign Language MNIST dataset in a low light environment and a bright light environment as well as at a 45 degree angle to the camera and directly at the camera. In the end our collected data will consist of 208 images. We will then begin our experiments by running these unaltered images in the classifier and getting the baseline accuracy of prediction. Then we will run some of the experiments we believe at this stage would help improve prediction accuracy which are listed below and we will record the scores of those experiments and then identify which experiments made the greatest improvements. Success of this project will be marked by the experiment's ability to identify improvements to the classifying process of the inputs.

The dataset used will be Sign Language MNIST for our training data set. It consists of 27,455 cases of sign language images of letters, each having a one-to-one mapping of labels from 0-25 representing the 26 alphabets. Each entry consists of 784 numbered pixels for a 28 x 28 pixel image. All images would be in grayscale with pixel values from 0 to 255, all cropped around the hand region of interest. These training images are pre-processed by resizing, applying filters 'Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite', 5% random pixelation, +/- 15% brightness/contrast, and rotation. 

#### Energy Minimization:
If we want all our images to be the same dimension, we can use seam carving in a similar way to how it was used in the first problem set. This way, rather than resizing traditionally, we will be able to resize the image without having to warp or alter the shape of the hand, making it easier for the image to be processed. The expected result of this experiment would be an image in which the hand has very similar proportions to the original picture in the resized picture, so that we will be able to test our model on images that look similar to each other. We will also run experiments to determine the best way to fit to the deformable contours. We will try the two different methods enumerated in the class notes to determine which one will provide better fitting to the hands. The expected result of this experiment would be that the dynamic programming method would fit to the contours with less loss and more consistency.

#### Filtering: 
Using filters would be a simple way to make the test images more uniform. Specifically, smoothing filters would be useful because we are mainly concerned with the outline of the hand, not the smaller details in the image. We could use a Gaussian filter for this purpose, and the expected result would be an image with edges that are still detectable but with less high frequency fluctuations. 

#### Segmentation: 
In order to best determine the main subject of the image which in this case should be the hand with the ASL symbol we will experiment with a couple of different segmentation methods and see which one will get us the best results consistently. The first method we will try to implement is the Graph-Cut Algorithm which will depict the image as a graph and remove the weakest links between the graphs isolating the main objects in the graph. We would also try the Hough Transform method to better identify the curves in the image and get a better isolation of the hand from the user input. These experiments will provide us with insights as to what the best way to extract the hand from the image would be. The expected result would be the optimal isolation of the hand, however, we are unsure of how the different orientations and environments of the images will determine which segmentation method works best.