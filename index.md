# American Sign Language Translator
Project for "CS4476 - Computer Vision" by Dheeraj Bandaru, Sichong Hua, Jing Xi Liu, Farouk Marhaba, and Kira Pancha.

## Problem Statement
Our team is working on a project that aims to parse American Sign Language (ASL) letters from a photo or video feed and convert it to text. Across different forms of communication, the mute community’s accessibility concerns have been historically underrepresented. Our project aims to provide an alternative for “speech-to-text” softwares for the mute community by giving users the ability to quickly and efficiently translate ASL letters to traditional English text.

The expected input into the system can be an image (single frame) where an ASL letter is present. Alternatively, it could be a video (multiple frames) where a string of ASL letters is parsed one letter at a time. After the frame(s) get processed through the translator, the corresponding English letters would be produced. The final output generated from the raw output can be represented in various forms, such as readable text, emojis, or even short-clipped audio.

## Approach
The technical approach we will take to get the results we desire will include building a classification model to differentiate between the different ASL signs. We will then build a series of image preprocessing methods implementing various Computer Vision techniques to enhance the user input for our model input, and finally create an output which will consist of a string of letters based on the user input. The classification model we plan to build to classify the different ASL signs will be the pretrained Inception V3 model which is a convolutional neural net that has been shown to classify images better and more efficiently than other widely available models like ImageNet. Inception V3 model is also easily implementable and can be trained on the datasets we find most relevant to our project. The next step will be to build the image preprocessing model which is where we will be experimenting with various different filters such as the Prewitt, Sobel, and Gaussian filters. We will also experiment with various edge detection methods and extraction methods like the graph-cut algorithm to better isolate the hand symbol and allow for better classification in our model. We will also use energy minimization techniques to allow for hands in different angles and lighting to be inputted and get reasonable outputs. The classifier model which will be used as a testing apparatus for our Computer Vision experiments we will have a baseline accuracy score based on just training with non-processed images and then we will run experiments using some of the various image preprocessing methods listed above to determine the best way we can improve the results of this device from a Computer Vision standpoint. 

## Experiments and Results
We would use Sign Language MNIST for our training data set. It consists of 27,455 cases of sign language images of letters, each having a one-to-one mapping of labels from 0-25 representing the 26 alphabets. Each entry consists of 784 numbered pixels for a 28 x 28 pixel image. All images would be in grayscale with pixel values from 0 to 255, all cropped around the hand region of interest. These training images are pre-processed by resizing, applying filters 'Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite', 5% random pixelation, +/- 15% brightness/contrast, and rotation. 
#### Energy Minimization:
If we want all our images to be the same dimension, we can use seam carving in a similar way to how it was used in the first problem set. This way, rather than resizing traditionally, we will be able to resize the image without having to warp or alter the shape of the hand, making it easier for the image to be processed. The expected result of this experiment would be an image in which the hand has very similar proportions to the original picture in the resized picture, so that we will be able to test our model on images that look similar to each other.
#### Filtering: 
Using filters would be a simple way to make the test images more uniform. Specifically, smoothing filters would be useful because we are mainly concerned with the outline of the hand, not the smaller details in the image. We could use a Gaussian filter for this purpose, and the expected result would be an image with edges that are still detectable but with less high frequency fluctuations. 
#### Segmentation: 
In order to best determine the main subject of the image which in this case should be the hand with the ASL symbol we will experiment with a couple of different segmentation methods and see which one will get us the best results consistently. The first method we will try to implement is the Graph-Cut Algorithm which will depict the image as a graph and remove the weakest links between the graphs isolating the main objects in the graph. We would also try the Hough Transform method to better identify the curves in the image and get a better isolation of the hand from the user input. These experiments will provide us with insights as to what the best way to extract the hand from the image would be.
