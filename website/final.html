<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Welcome file</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="american-sign-language-translator">American Sign Language Translator</h1>
<p>Project for “CS4476 - Computer Vision” by Dheeraj Bandaru, Sichong Hua, Jing Xi Liu, Farouk Marhaba, and Kira Pancha.</p>
<p><A HREF="./proposal.html">Project Proposal</A></p>
<p><A HREF="./midtermUpdate.html">Midterm Update</A></p>
<h2 id="abstract">Abstract</h2>
<p>This project aims to parse American Sign Language (ASL) letters from a photo or video feed and convert it to text and to do accomplish this we built a classification model using Inception V3 to differentiate between the different ASL signs and then implemented a series of image preprocessing methods using various Computer Vision techniques to enhance the user input for our classifier. We obtained an initial baseline accuracy score based on just training with non-processed images, and then we ran experiments using edge detection and accentuation, contour lines, and Filtering and Threshold to determine the best way we can improve the results of this classification from a Computer Vision standpoint.</p>
<h2 id="teaser-figure">Teaser Figure</h2>
<p><img src="https://lh4.googleusercontent.com/ozla_6BC60qMYsM6CruT4skZ_Ef7KQz7LZnA0BVagoou8WbdaISt7k6MmQplhyPBZW7L5p4GTJHjhUXZIxZtO-Jp-qd47AqFsVsNIRLw  " style="margin:10px;width:8000px;height:200px;" alt=""></p>
<h2 id="Introduction">Introduction</h2>
<p>Our team worked on a project that aims to parse American Sign Language (ASL) letters from a photo or video feed and convert it to text. Across different forms of communication, the mute community’s accessibility concerns have been historically underrepresented. Our project aims to provide an alternative for “speech-to-text” software for the mute community by giving users the ability to quickly and efficiently translate ASL letters to traditional English text.</p>
<p>The expected input into the system is an image (single frame) where an ASL letter is present. After the frame(s) get processed through the translator, the corresponding English letters would be produced. The final output generated from the raw output can be represented in various forms, such as readable text.</p>
<h2 id="approach">Approach</h2>
<h4 id="model">Model</h4>
<p>To reach our end goal of improving upon current American Sign Language Translators we needed a system to test the experiments that would give us a fair estimate of how our alterations to the given images would affect the accuracy with which the computer can interpret the various images of ASL signs. In order to create a baseline test, we decided to use an Inception V3 model that was pre-trained on ImageNet Images to save a lot of computation time on our side in terms of having predetermined optimal weights for each layer of the neural network. This decision was made due to the fact we are mainly interested in improving the predictions by processing the image itself using various methods rather than experimenting with the various parameters of the classifier.  As a result of this design consideration, we were able to use a dataset and apply transfer learning to get results for the ASL images within 3 hours. We were able to find a dataset that was used to train on this model from https://github.com/loicmarie/sign-language-alphabet-recognizer. This allowed us to get a baseline result for images that weren’t pre-processed using methods from Computer Vision. The Inception V3 model was then also trained based on the initial preprocessed images that are described in more detail in future sections to and then we compared the results between the two datasets (raw vs. preprocessed images).</p>
<p>The method used to classify these new images using the Inception V3 model is called transfer learning and because the images we are passing are a new class of images we have to create a new layer on top of the Inception V3 model. By using the name of the subfolder of the image as the label of the image we start the transfer learning process by navigating to the images. We then go ahead and add a new layer for the new class of the images. Then we will calculate the bottleneck layer values for each image which is a layer in a TensorFlow neural network that has fewer neurons than the layer below or above it. Bottleneck layer values help speed up the computation process because we will be able to optimally compress the feature representations to fit the space we have. After calculating this value we cache the values so we can quickly retrieve the value for future reference. Then we go through the evaluation step which will train the layer for the image class we have requested and we repeat this step for however many training steps the user has requested. In our case, we ran the model with 2000 training steps as the project referenced before used this amount of steps and received successful results. Then we output the results to graphs that can be used later for prediction.</p>
<p>Some of the technical challenges we ran into when running this transfer learning method included having to accommodate for the fact the code was written in Python 2.7 which meant we needed to set up a virtual environment in order to be able to properly manage the packages the code needed. The outputs of the code also ended up being far too large to go onto GitHub which meant we needed to find another file sharing solution.</p>
<h2 id="experiments-and-results">Experiments</h2>
<h3 id="dataset">Dataset</h3>
<p>We wanted to use this Sign Language MNIST dataset from Kaggle to train and test our model originally. However, this data is structured as a CSV file for 250x250 pixel images with each pixel value represented as a grayscale value between 0-255. Instead, we used the ASL Alphabet dataset to train our model. The training data consists of 87,000 images that are 200x200 pixels. They are separated into 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE, and NOTHING. The testing dataset only consists of 29 images, one for each class. This dataset is organized with each letter in a separate directory. Within each of these directories, the images of each letter are sorted in order of decreasing similarities, so that the images with smaller indices look more alike.</p>
<p>Each training and test case represents a label as a one-to-one match with one letter from the alphabet. The original images came from users doing the gestures with different backgrounds, but to expand the dataset these images were processed with a pipeline called ImageMagick.</p>
<h3 id="image-processing">Image Processing</h3>
<p>Our goal was to compare the results between models trained with the original dataset versus preprocessed versions of the dataset. In order to do this, we first had to apply various segmentation, filtering, and contouring techniques to the images to create our three separate preprocessing image pipelines (to be compared to the raw image pipeline). For all of the pipelines, we used the same 300 randomly selected original images for each character in order to eliminate any errors associated with image selection and classification. The following sections highlight our three separate experimental image preprocessing pipelines:</p>
<h5 id="segmentation">Pipeline 1 - Edge Detection and Accentuation</h4>
<p>Our first image preprocessing pipeline revolved around edge detection and accentuation, with examples shown in Figure 1 and 2. First, we removed a 5-pixel border from each edge of the image. We found that in many images, there was a border that was notably either darker or lighter than its surroundings, so we removed a thin border to enhance focus on the subject of the image. Furthermore, it was rare that an image was so close up that removing this border would have cut into the subject of the image, making it a reasonable measure to take. This cropping was used throughout all of our pipelines.</p>
<p>Next, we performed Canny edge detection on the image. Before doing this, we converted the image to grayscale and increased the contrast so edges would be more apparent. Then, we applied a median filter on the image before using the Canny edge detector. We preferred using the non-linear median filter over a Gaussian filter because it reduces noise while keeping edges relatively sharp, which was our goal. Using a Gaussian filter blurs the whole image and reduces contrast overall, which would effectively reduce noise but also counteract the processing we had just done to increase contrast in the image. We then took the edges detected by the Canny edge detector, and made them stand out by coloring them bright yellow on the original image.</p>
<h5 id="contour">Pipeline 2 - Contour Lines</h4>
<p>For our second preprocessing pipeline, we wanted to get a distinct outline of the subject of each image to see how that would affect the training and testing. Examples of this preprocessing can be seen in Figure 3 and 4. We started off by using a Gaussian blur to smooth the image. Then, we used the Canny edge detector to find edges in the image. We used a sigma value of 1.0 since the images we ran it on were relatively low-resolution. To find the contours, we used OpenCV’s findContours method, and we were able to draw the contour lines on the image. In order to increase accuracy, the last thing we did was crop 5 pixels off of each border. This was based on the observation that in nearly every image there were contour lines along the border that were irrelevant to the subject of the image, so we cropped them out.</p>
<p>An interesting side note about the contour lines along the edge of the image is that in many cases they were actually the same contour line that outlined the subject of the image. Because of this, it was difficult to filter out these irrelevant contour lines because they actually turned into the same line as a relevant contour line. So, it seemed like the logical solution was to simply crop out these edges, especially considering in most cases it did not interfere with the subject of the image.</p>
<h5 id="contour">Pipeline 3 - Filtering and Threshold</h4>
<p>For our third preprocessing pipeline, we decided to experiment with various filters and thresholding methods that were available to isolate the subject of our images and create the best possible image for our classification system. Examples of this preprocessing can be seen in Figure 5 and 6.</p>
<p>The initial idea we wanted to experiment with for this pipeline had to do with deformable contours using active contours. To use active contours we needed to establish a circle for which the energy is minimized within and because we can’t go through and manually select a circle for every single image in the dataset we decided to use Hough Transform to detect a circle with the greatest peak to give us a rough idea of where the subject of the image is located in the image. Prior to running the Hough Transform we made sure to grayscale the image and crop it to be the same size like the other pipelines. The results of this Hough Transforms and the corresponding active contours were not very encouraging.</p>
<p>After those disappointing early results we decided to see if removing the background using a filtering and thresholding method would make it easier for the Hough Transform. In order to create the best thresholded image we once again started with the gray, cropped image. The first thresholding method we experimented with was threshold minimum which is computed based on the minimum from the image histogram. This yielded in better results however the image still had a lot of the background included in the background. In order to combat this issue, especially when the background had two tones of color, we decided to implement a filter that would accentuate edges and use that filtered image to threshold. The filter selected for this was the Sobel filter as it works by generating the gradient of image intensity which is the information needed to best identify the edges in a given image. With the Sobel filter applied to the image and a gaussian blur applied to clean up the image, we experimented with more thresholding methods to see which visual output would best isolate the hand. Threshold mean, which is a threshold value based on the mean of the grayscale values, was adequate but not great when the image was darker and the background and foreground had similar values but other parts of the background did not have similar values. The thresholding method that worked best was thresholding based on Otsu’s method. Otsu’s method is often used for thresholding because it uses the intensity histogram and clustering methods to differentiate between the foreground and the background. The resulting image after this is a binary image where the foreground (most of the time the hand) is black and the rest is white.</p>
<p>Using these thresholded and filtered images we went to try the Hough Transform circle detection again however once again the circle being detected was not corresponding with the subject of the image, so we decided with the current resources available to us we could not perform active contours for the deformable contours. However, we did want to see if the filtered and thresholded images would yield in better results. The results were not great and the most likely reason for this failure is the fact that the images in our dataset had hands at different locations and with the binary image removing all additional data points like shadows and the filling of the hand and leaving just the edges we were giving the classifier less information even if the information was more relevant to what we were trying to predict.</p>
<h2 id="experiments-and-results">Testing and Results</h2>
<p>Due to the large amount of data we have, we were not able to process all images in time, and train all datasets. Thus, out of the 3000 images we have for each letter, we initially used the first 100 in ascending order. As mentioned above, the images for each letter in this dataset is organized with decreasing similarities, so that the images with small indices look alike. These images are very similar in their position of the hand, lighting, and image features in general, so the model yields a very high accuracy for both the original and preprocessed images. Thus we increased the dataset size to 300 images per letter and randomized the selection of images so that the dataset contains images from different angles and lightings.</p>
  <BLOCKQUOTE>Final test accuracy with 300 random images for each letter<BR>
    Normal images: 83.4%<BR>
    Pipeline 1 (Edge Detection and Accentuation): 86.1%<BR>
    Pipeline 2 (Contour Lines): 77.2%<BR>
    Pipeline 3 (Filtering and Thresholding): 68.1%<BR>
</BLOCKQUOTE>
<p>We can see a slight increase in final test accuracy with the processed images using pipeline 1 with canny edge detector. Other pipelines yield a lower accuracy than the original dataset, however. Possible reasons for this could be inconsistent lightings of backgrounds. For examples, in many images, we can see a clear line between the wall and the ceiling in the background, where our pre-processing approaches would consider significant information that is needed to be included in the processed images.</p>
<h2 id="qualitative-results">Qualitative Results</h2>
<p><img src="https://lh3.googleusercontent.com/BvCyABD6xm5jom_1KtswjZzEiKhbxF58OX25NXag1SX3jPEJaufeSKGNPgr-tTB9EcSBsCNE69IzrypLMTMOgYh3vmn1fHS-lAwFH5Q" style="margin:10px;width:200px;height:200px;" alt=""><img src="https://lh4.googleusercontent.com/cp463_4OSUZuLndjBXHT7irJ_og79mAdn53dxR3qGWwMH4Sf6LvT5mcV1Abtkmmsgqq7OTvtLlDBdEzijY820XOYSNLFL7SeU3jH_1Q" style="margin:10px;width:200px;height:200px;" alt=""></p>
<p>Figure 1 (left). Raw sample image of letter A.</p>
<p>Figure 2 (right). Preprocessed image from Figure 1 using Pipeline 1.</p>
<p><img src="https://lh5.googleusercontent.com/8XXJ13StVFj2UHXDuiRXvlnB57f1UhkiobrBENCHJTeyGwVeikG8-QGnCx6DtMBWrGsZk9zX_KCluPGHExgyQiZqypOw9zHU3NolfFPf" style="margin:10px;width:200px;height:200px;" alt=""><img src="https://lh4.googleusercontent.com/sfP2bKmVXzRE7E3WE2q7Tc4TceWNfZGpxZT2aNPN7V6ws00A7OAm1a5hm6wxSn43OU7uv7seFwY3UnWgQgbQVRe3tZKsS9ojzPjg_5PeT8NASWr-SPAPw0dxT6lat_Dtzq6ehc4P" style="margin:10px;width:200px;height:200px;" alt=""></p>
<p>Figure 3 (left). Raw sample image of letter A</p>
<p>Figure 4 (right). Preprocessed image from Figure 3 using Pipeline 2.</p>
<p><img src="https://lh3.googleusercontent.com/WRi4C4pjIg1MYr_V1EN-ES5x0FSyunwMyL4Hmsy5ywvZW50VWmXYMCPe6kYeTLOwbhZjQmJRuEK4LuUdDHHYgNNRKrcghCTuwYriPgF0t0HwpfT72LLuqYPth9t6doiVHezUAAck" style="margin:10px;width:200px;height:200px;" alt=""><img src="https://lh3.googleusercontent.com/DdIx4BLq_8wIqIG4ekJILV6eXmPada_C4OyiQDrYIleTZbZAisKE0PtdV4KRPptA6SiUZOiMrbIsT6kLGcxFvjejmYMCEUWku5iL4dI" style="margin:10px;width:200px;height:200px;" alt=""></p>
<p>Figure 5 (left). Raw sample image of letter A.</p>
<p>Figure 6 (right). Preprocessed image from Figure 5 using Pipeline 3.</p>
<p><img src="https://lh6.googleusercontent.com/ujumgotJHkR-lwfK1gqLYJoz6MwfEr21mn7YBhl6ZyS7jH_C9bbhHuaQxP4laBsJF8cQqg2PZ0z-jEm_hJ7zcOfENFx_sdX7OWXdJVFR" style="margin:10px;width:200px;height:200px;" alt=""><img src="https://lh3.googleusercontent.com/b_in9cKd1xdOhVrNsUCQjbhQtSKS3QKlvx45sSXPCKJmuTLehi7l1iebMDU_Dkq0jIO3-H8dwKDtEJ9JhKxbMpCYrC0vSkmx39OBFqQ" style="margin:10px;width:200px;height:200px;" alt=""></p>
<p>Figure 7 (left). Hough Transform Circle Detection for grayscale image of letter V.</p>
<p>Figure 8 (right). Hough Transform Circle Detection for preprocessed image of letter V using Pipeline 3.</p>

<h2 id="conclusion-and-future-work">Conclusion and future work</h2>
<p>In conclusion, we were very pleased with our progress and results for our ASL letter classification project. We were able to build a classification model and image preprocessing pipeline that classified letters with higher accuracy (86.1%) compared to a classification model that used the raw images (83.4%), using computer vision techniques focused on edge detection and edge accentuation. We were also able to experiment and experience image preprocessing pipelines that did not outperform raw image classification, which provided valuable insights on the limitations of image preprocessing within the context of classification, as well as information on key aspects and features of images that need to be retained for successful classification.</p>
<p>Reflecting on our project, and future work we would want to look into there are many avenues we would like to continue to work on and improve as our results from our current experiments have been a little underwhelming.</p>
<p>One of the primary future work we would like to focus on moving forward includes the energy minimization using active contouring. As seen from our qualitative results (Figure 8) we have not had much success finding circles within an image that would focus on the subject of the image which in this case would be the hand. Some other ideas we could explore to avoid this issue could be finding multiple circles from Hough Transform and taking an average or an aggregate of the circles to ensure we keep all parts of the subject within the circle. We could also try other methods such as using edge detection and then altering the image to bring all the hands in the images to the same location and have a constant circle. All of these ideas, if successful, would give us a better idea of how impactful active contouring could be to our preprocessing methods.</p>
<p>Another potential future work would be the use of the connected components algorithm to better identify the foreground and the subject of the image as opposed to just using the edges of the images. By using the pixels that share similar intensities and are neighbors of each other we can hopefully identify each hand as a component and isolate the hand in that way and then use that isolated hand for other preprocessing methods without losing the details of the actual sign indicated by the person in the picture.</p>
<p>In the future we would also like to expand this project to take custom inputs for the classifier which will lead to more edge case testing for lighting and positioning to verify our preprocessing methods work for images outside of the dataset being currently used.</p>
<h2 id="references">References</h2>
<ol>
    <li><a href="https://github.com/xuetsing/image-classification-tensorflow/blob/master/train.py">https://github.com/xuetsing/image-classification-tensorflow/blob/master/train.py</a></li>
    <li><a href="https://github.com/loicmarie/sign-language-alphabet-recognizer">https://github.com/loicmarie/sign-language-alphabet-recognizer</a></li>
    <li><a href="https://www.kaggle.com/grassknoted/asl-alphabet">https://www.kaggle.com/grassknoted/asl-alphabet</a> The dataset we used for training and initial testing</li>
    <li><a href="https://scipy-lectures.org/packages/scikit-image/index.html">https://scipy-lectures.org/packages/scikit-image/index.html</a> Scikit-image filtering we used</li>
    <li><a href="https://homepages.inf.ed.ac.uk/rbf/HIPR2/label.htm">https://homepages.inf.ed.ac.uk/rbf/HIPR2/label.htm</a> Connected Components Algorithm</li>
    <li><a href="http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html">http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html</a> Otsu Thresholding</li>
    <li><a href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.active_contour">https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.active_contour</a> Active Contour Model</li>
  </ol>
</div>
</body>

</html>
