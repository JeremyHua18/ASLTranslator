<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Welcome file</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="american-sign-language-translator">American Sign Language Translator</h1>
<p>Project for “CS4476 - Computer Vision” by Dheeraj Bandaru, Sichong Hua, Jing Xi Liu, Farouk Marhaba, and Kira Pancha.</p>
<h2 id="midterm-update">Midterm Update</h2>
<h3 id="model">Model</h3>
<p>To reach our end goal of improving upon current American Sign Language Translators we needed a system to test our experiments that would give us a fair estimate on how our alterations to the given images would affect the accuracy with which the computer can interpret the various images of ASL signs. In order to create a baseline test, we decided to use an Inception V3 model that was pre trained on ImageNet Images to save a lot of computation time on our side in terms of having predetermined optimal weights for each layer of the neural network. This decision was made due to the fact we are mainly interested in improving the predictions by processing the image itself using various methods rather than experimenting with the various parameters of the classifier. As a result of this design consideration we were able to use a dataset and apply transfer learning to get results for the ASL images within 3 hours. We were able to find a dataset that was used to train on this model from <a href="https://github.com/loicmarie/sign-language-alphabet-recognizer.This">https://github.com/loicmarie/sign-language-alphabet-recognizer.</a> This allowed us to get a baseline result for images that weren’t pre processed using methods from Computer Vision. The Inception V3 model was then also trained based on the initial preprocessed images that are described in more detail in future sections to and then we compared the results between the two datasets (raw vs. preprocessed images).</p>
<p>The method used to classify these new images using the Inception V3 model is called transfer learning and because the images we are passing are a new class of images we have to create a new layer on top of the Inception V3 model. By using the name of the subfolder of the image as the label of the image we start the transfer learning process by navigating to the images. We then go ahead and add a new layer for the new class of the images. Then we will calculate the bottleneck layer values for each image which is a layer in a tensorflow neural network that has less neurons than the layer below or above it. Bottleneck layer values help speed up the computation process because we will be able to optimally compress the feature representations to fit the space we have. After calculating this value we cache the values so we can quickly retrieve the value for future reference. Then we go through the evaluation step which will train the layer for the image class we have requested and we repeat this step for however many training steps the user has requested. In our case we ran the model with 2000 training steps as the project referenced before used this amount of steps and received successful results. Then we output the results to graphs that can be used later for prediction.</p>
<p>Some of the technical challenges we ran into when running this transfer learning method included having to accommodate for the fact the code was written in Python 2.7 which meant we needed to set up a virtual environment in order to be able to properly manage the packages the code needed. The outputs of the code also ended up being far too large to go onto GitHub which meant we needed to find another file sharing solution.</p>
<h3 id="dataset">Dataset</h3>
<p>We wanted to use this <a href="https://www.kaggle.com/datamunge/sign-language-mnist">Sign Language MNIST</a> dataset from Kaggle to train and test our model originally. However, this data is structured as a CSV file for 250x250 pixel images with each pixel value represented as a grayscale value between 0-255. Instead, we used the <a href="https://www.kaggle.com/grassknoted/asl-alphabet">ASL Alphabet</a> dataset to train our model. The training data consists of 87,000 images that are 200x200 pixels. They are separated into 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING. The testing dataset only consists of 29 images, one for each class. This dataset is organized with each letter in a separate directory. Within each of these directories, the images of each letter is sorted in order of decreasing similarities, so that the images with smaller indices look more alike.</p>
<p>Each training and test case represents a label as a one-to-one match with one letter from the alphabet. The original images came from users doing the gestures with different backgrounds, but to expand the dataset these images were processed with a pipeline called ImageMagick.</p>
<h3 id="segmentation">Segmentation</h3>
<p>Our goal was to compare the results between models trained with the original dataset versus a preprocessed version of the dataset. In order to do this we first had to apply various segmentation techniques to the images to create our preprocessed images. We mainly used methods from scikit image for applying filters, detecting edges, and creating masks. The process for each image went as follows:</p>
<p>First, we removed a 5 pixel border from each edge of the image. We found that in many images, there was a border that was notably either darker or lighter than its surroundings, so we removed a thin border to enhance focus on the subject of the image. Furthermore, it was rare that an image was so close up that removing this border would have cut into the subject of the image, making it a reasonable measure to take.</p>
<p>We then performed Canny edge detection on the image. Before doing this, we converted the image to grayscale and increased the contrast so edges would be more apparent. Then, we applied a median filter on the image before using the Canny edge detector. We preferred using the non-linear median filter over a Gaussian filter because it reduces noise while keeping edges relatively sharp, which was our goal. Using a Gaussian filter blurs the whole image and reduces contrast overall, which would effectively reduce noise but also counteract the processing we had just done to increase contrast in the image. We then took the edges detected by the Canny edge detector, and make them stand out by coloring them yellow on the original image.</p>
<p>The second part of our image segmentation pipeline was not used in training the models for this update, but is intended for use in the next project update. To find the lightest and darkest spots in the image, we used a threshold with the grayscale images. We use a separate threshold for finding light and dark spots so that values in the middle are not classified as either. We then use these light and dark spots to create a watershed image displaying the topography of the image. In the next project update, our goal is to use this watershed to identify and remove the background. The goal is to eventually be able to remove the background of the images so that when the model is being trained it is apparent what the subject of the image is.</p>
<p><img src="https://lh3.googleusercontent.com/e3tvmLq06ir04VQkB-BODjqKCo5JLbJQV4HSIAzDgJ4Y8H0DtFMzTW_KeJ3AA8kEKfX8VkCk_A5Y3hYerQ_mEFIXAZynHYEd83rmn6W6Hqy8YpxirHfSAmuwxDb43yqdajux6CQ" alt=""></p>
<p>Figure 1. Original image of sign “A”</p>
<p><img src="https://lh3.googleusercontent.com/n1SBVCr3GrDbiAiU_0ZdhvBJTKnN8jRyna9u0OObCg90SJ3ckHoN4CC65l6FtqRgitQJO5i_SLusJ3oRmJ9mo1dbmt0BKHZ_PO_elq1IltO7_Y94wAk1-CaE4SkinF5-sSfJBps" alt=""></p>
<p>Figure 2. Segmented image of sign “A”</p>
<p><img src="https://lh4.googleusercontent.com/Os3nFODc9btO_G0DW9t3rFz0vhwyy7tX90mBEXEQir-hBBZhR-wl-UhM2s5iwL3TZER6_IiR5LmMYa4UHugAZmZqbPWXp-fkC4R_xYgQis_ktpy8e72ZhpOjYyT0u69ny_rDx9U" alt="">Figure 3. Results from training with original dataset</p>
<p><img src="https://lh3.googleusercontent.com/cVtFivrM7hF5rs7mLDqy64U3CLXXUUVitgWeFjZ2q6jPJPunvXEGdvLZkL6InZPslCnRY-b5Rmzy2AvA1uH4YQ2pTrQqFe9yZmCK4qa6Yd1fYQ_v9GHMOfxiIzH0qATAjHUUVY8" alt=""></p>
<p>Figure 4. Results from training with preprocessed dataset</p>
<h3 id="analysis-on-results">Analysis on Results</h3>
<p>Due to the large amount of data we have, we were not able to process all images in time, and train all datasets. Thus, out of the 3000 images we have for each letter, we only used the first 300 in ascending order. As mentioned above, the images for each letter in this dataset is organized with decreasing similarities, so that the images with small indices look alike. These images are very similar in their position of the hand, lighting, and image features in general, so the model yields a very high accuracy for both sets of images. Yet, we can still see a slight increase in final test accuracy with the processed images. We expect the numbers to be more significant as we increase the training and testing data size.</p>
<p>Due to time restrictions we were not able to plot and understand the optimal batch size for the model to make it more efficient. We also have not looked into how having preprocessed images can help reduce the amount of steps required to reach the accuracy we have reached currently of roughly 99%. We also want to look into the other metrics to determine whether the model has overfitted the dataset or if the preprocessed images have truly outperformed the raw images.</p>
<h3 id="conclusion-and-next-steps">Conclusion and Next Steps</h3>
<p>In conclusion, we are currently excited with the progress we made and although we only made a slight improvement in classification of about 0.4% it is still significant and is an indicator with more preprocessing and training we could see real improvements in classification. Some future steps we are looking to take include improvements in segmentation and masking as well as adding other methods to better identify the subject of the image. We are also going to test using custom inputs that will include more noise and see how the model performs in this case. We will also look into whether there are more efficient ways to run the classification process.</p>
<h3 id="references">References</h3>
<ol>
<li><a href="https://github.com/xuetsing/image-classification-tensorflow/blob/master/train.py">https://github.com/xuetsing/image-classification-tensorflow/blob/master/train.py</a></li>
<li><a href="https://github.com/loicmarie/sign-language-alphabet-recognizer">https://github.com/loicmarie/sign-language-alphabet-recognizer</a></li>
<li><a href="https://www.kaggle.com/grassknoted/asl-alphabet">https://www.kaggle.com/grassknoted/asl-alphabet</a> The dataset we used for training and initial testing</li>
<li><a href="https://scipy-lectures.org/packages/scikit-image/index.html">https://scipy-lectures.org/packages/scikit-image/index.html</a> Scikit-image filtering we used</li>
</ol>
</div>
</body>

</html>
